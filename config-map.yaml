apiVersion: v1
kind: ConfigMap
metadata:
  name: preprocess-script
  namespace: argowf
data:
  preprocessing.py: |
    import pandas as pd
    import mlflow
    import mlflow.sklearn
    from sklearn.preprocessing import LabelEncoder
    from sklearn.model_selection import train_test_split
    import os

    # Set MLflow tracking URI (use local filesystem for now)
    mlflow.set_tracking_uri("file:///opt/ml/processing/mlruns")
    
    # Start MLflow run
    with mlflow.start_run(run_name="preprocessing"):
        df = pd.read_csv('/opt/ml/processing/input/WA_Fn-UseC_-Telco-Customer-Churn.csv')
        
        # Log dataset info
        mlflow.log_param("original_dataset_shape", df.shape)
        mlflow.log_param("original_columns", list(df.columns))
        
        df = df.drop(['customerID'], axis=1)
        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)

        le = LabelEncoder()
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = le.fit_transform(df[col])

        X = df.drop('Churn', axis=1)
        y = df['Churn']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Log preprocessing parameters
        mlflow.log_param("test_size", 0.2)
        mlflow.log_param("random_state", 42)
        mlflow.log_param("processed_features", list(X.columns))
        mlflow.log_metric("train_samples", len(X_train))
        mlflow.log_metric("test_samples", len(X_test))

        train = pd.concat([y_train, X_train], axis=1)
        test = pd.concat([y_test, X_test], axis=1)

        # Ensure output directories exist
        os.makedirs('/opt/ml/processing/output/train', exist_ok=True)
        os.makedirs('/opt/ml/processing/output/test', exist_ok=True)

        train.to_csv('/opt/ml/processing/output/train/train.csv', index=False)
        test.to_csv('/opt/ml/processing/output/test/test.csv', index=False)
        
        print("Preprocessing completed with MLflow tracking")

  xgboost_script.py: |
    import argparse
    import os
    import pandas as pd
    import xgboost as xgb
    import mlflow
    import mlflow.xgboost
    from sklearn.metrics import accuracy_score, roc_auc_score

    # Parse arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--output-dir", type=str, default="/opt/ml/processing/hpo")
    args = parser.parse_args()

    # Set MLflow tracking URI
    mlflow.set_tracking_uri("file:///opt/ml/processing/mlruns")
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Load data
    train_data = pd.read_csv("/opt/ml/processing/output/train/train.csv")
    test_data = pd.read_csv("/opt/ml/processing/output/test/test.csv")

    # Prepare features and target
    target = "Churn"
    if target in train_data.columns:
        # Start MLflow run for training
        with mlflow.start_run(run_name="xgboost_training"):
            features = train_data.drop(columns=[target])
            test_features = test_data.drop(columns=[target])
            
            # Model parameters
            params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'random_state': 42
            }
            
            # Log parameters
            mlflow.log_params(params)
            
            # Train model
            model = xgb.XGBClassifier(**params)
            model.fit(features, train_data[target])
            
            # Make predictions
            train_predictions = model.predict(features)
            train_probs = model.predict_proba(features)[:, 1]
            test_predictions = model.predict(test_features)
            test_probs = model.predict_proba(test_features)[:, 1]
            
            # Calculate metrics
            train_acc = accuracy_score(train_data[target], train_predictions)
            train_auc = roc_auc_score(train_data[target], train_probs)
            test_acc = accuracy_score(test_data[target], test_predictions)
            test_auc = roc_auc_score(test_data[target], test_probs)
            
            # Log metrics
            mlflow.log_metric("train_accuracy", train_acc)
            mlflow.log_metric("train_auc", train_auc)
            mlflow.log_metric("test_accuracy", test_acc)
            mlflow.log_metric("test_auc", test_auc)
            
            # Log model
            mlflow.xgboost.log_model(model, "model")
            
            # Save model locally
            model.save_model(f"{args.output_dir}/model.xgb")
            
            # Save metrics to file
            with open(f"{args.output_dir}/metrics.txt", "w") as f:
                f.write(f"Train Accuracy: {train_acc}\n")
                f.write(f"Train AUC: {train_auc}\n")
                f.write(f"Test Accuracy: {test_acc}\n")
                f.write(f"Test AUC: {test_auc}\n")
            
            print(f"Model saved to {args.output_dir}/model.xgb")
            print(f"Train Accuracy: {train_acc}")
            print(f"Train AUC: {train_auc}")
            print(f"Test Accuracy: {test_acc}")
            print(f"Test AUC: {test_auc}")
            print(f"MLflow run ID: {mlflow.active_run().info.run_id}")
    else:
        print(f"Target column '{target}' not found in data")

  evaluation.py: |
    import pandas as pd
    import numpy as np
    import xgboost as xgb
    import json
    import logging
    import os
    import mlflow
    import mlflow.xgboost
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    if __name__ == '__main__':
        logger.info("Starting evaluation script with MLflow tracking.")
        
        # Set MLflow tracking URI
        mlflow.set_tracking_uri("file:///opt/ml/processing/mlruns")

        with mlflow.start_run(run_name="model_evaluation"):
            # Load test data
            try:
                logger.info("Loading test data from /opt/ml/processing/output/test/test.csv")
                test_data = pd.read_csv('/opt/ml/processing/output/test/test.csv')
                X_test = test_data.drop('Churn', axis=1)
                y_test = test_data['Churn']
                logger.info(f"Test data loaded successfully. Shape: {test_data.shape}")
            except Exception as e:
                logger.error(f"Failed to load test data: {e}")
                raise

            # Load model
            try:
                logger.info("Loading model from /opt/ml/processing/hpo/model.xgb")
                model = xgb.XGBClassifier()
                model.load_model('/opt/ml/processing/hpo/model.xgb')
                logger.info("Model loaded successfully.")
            except Exception as e:
                logger.error(f"Failed to load model: {e}")
                raise

            # Make predictions
            try:
                logger.info("Making predictions on test data.")
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                logger.info("Predictions completed successfully.")
            except Exception as e:
                logger.error(f"Failed to make predictions: {e}")
                raise

            # Calculate metrics
            try:
                logger.info("Calculating evaluation metrics.")
                metrics = {
                    'accuracy': float(accuracy_score(y_test, y_pred)),
                    'precision': float(precision_score(y_test, y_pred)),
                    'recall': float(recall_score(y_test, y_pred)),
                    'f1': float(f1_score(y_test, y_pred)),
                    'roc_auc': float(roc_auc_score(y_test, y_pred_proba)),
                    'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()
                }
                
                # Log metrics to MLflow
                for metric_name, metric_value in metrics.items():
                    if metric_name != 'confusion_matrix':
                        mlflow.log_metric(f"eval_{metric_name}", metric_value)
                
                logger.info(f"Evaluation metrics calculated: {metrics}")
            except Exception as e:
                logger.error(f"Failed to calculate metrics: {e}")
                raise

            # Save metrics
            try:
                os.makedirs('/opt/ml/processing/evaluation', exist_ok=True)
                output_path = '/opt/ml/processing/evaluation/evaluation.json'
                logger.info(f"Saving evaluation metrics to {output_path}")
                with open(output_path, 'w') as f:
                    json.dump(metrics, f)
                logger.info("Evaluation metrics saved successfully.")
                logger.info(f"MLflow run ID: {mlflow.active_run().info.run_id}")
            except Exception as e:
                logger.error(f"Failed to save evaluation metrics: {e}")
                raise
