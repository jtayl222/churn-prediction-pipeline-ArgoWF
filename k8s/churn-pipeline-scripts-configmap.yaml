apiVersion: v1
data:
  evaluation_script.py: "import xgboost as xgb\nimport pandas as pd\nimport os\nimport
    logging\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score,
    recall_score, f1_score\nimport json\nimport argparse\nimport mlflow\n\n# Configure
    logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s
    - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef evaluate_model(model_path,
    valid_data_path, metrics_output_path, mlflow_experiment_name, training_run_id=None):\n
    \   \"\"\"\n    Loads a trained model, evaluates it on validation data, and logs
    metrics.\n    \"\"\"\n    logger.info(f\"Starting model evaluation.\")\n    logger.info(f\"MLflow
    Experiment Name: {mlflow_experiment_name}\")\n    if training_run_id:\n        logger.info(f\"Associated
    Training MLflow Run ID: {training_run_id}\")\n\n    # --- MLflow Setup ---\n    #
    MLFLOW_TRACKING_URI and S3 credentials should be set as environment variables
    in the Argo workflow\n    try:\n        mlflow.set_experiment(mlflow_experiment_name)\n
    \       logger.info(f\"Using MLflow experiment: {mlflow_experiment_name}\")\n
    \   except Exception as e:\n        logger.error(f\"Could not set MLflow experiment
    '{mlflow_experiment_name}': {e}\")\n        # Attempt to create if not exists\n
    \       try:\n            logger.info(f\"Attempting to create MLflow experiment:
    {mlflow_experiment_name}\")\n            mlflow.create_experiment(mlflow_experiment_name)\n
    \           mlflow.set_experiment(mlflow_experiment_name)\n        except Exception
    as e_create:\n            logger.error(f\"Failed to create or set MLflow experiment:
    {e_create}\")\n            # Decide if you want to proceed without MLflow or raise
    error\n\n    # Start a new MLflow run for evaluation, or use the parent training
    run if ID is provided\n    # For simplicity here, we'll create a new run.\n    #
    To nest it under the training run, you'd use:\n    # with mlflow.start_run(run_id=training_run_id,
    nested=True, run_name=\"evaluation\") if training_run_id else mlflow.start_run(run_name=\"evaluation_run\")
    as run:\n    \n    # For now, let's create a distinct evaluation run, but tag
    it if training_run_id is available\n    with mlflow.start_run(run_name=\"model_evaluation_run\")
    as run:\n        eval_run_id = run.info.run_id\n        logger.info(f\"Evaluation
    MLflow Run ID: {eval_run_id}\")\n        if training_run_id:\n            mlflow.set_tag(\"training_run_id\",
    training_run_id)\n        mlflow.log_param(\"model_path\", model_path)\n        mlflow.log_param(\"validation_data_path\",
    valid_data_path)\n\n        # Load the trained model\n        logger.info(f\"Loading
    model from {model_path}\")\n        try:\n            model = xgb.XGBClassifier()\n
    \           model.load_model(model_path)\n            logger.info(\"Model loaded
    successfully.\")\n        except Exception as e:\n            logger.error(f\"Failed
    to load model from {model_path}: {e}\")\n            mlflow.set_tag(\"evaluation_status\",
    \"failed_model_load\")\n            raise\n\n        # Load validation data\n
    \       logger.info(f\"Loading validation data from {valid_data_path}\")\n        try:\n
    \           valid_data = pd.read_csv(valid_data_path)\n            X_valid = valid_data.iloc[:,
    1:]  # Assuming first column is target\n            y_valid = valid_data.iloc[:,
    0]\n            logger.info(f\"Validation data loaded. Shape: X_valid {X_valid.shape},
    y_valid {y_valid.shape}\")\n        except Exception as e:\n            logger.error(f\"Failed
    to load validation data from {valid_data_path}: {e}\")\n            mlflow.set_tag(\"evaluation_status\",
    \"failed_data_load\")\n            raise\n\n        # Make predictions\n        logger.info(\"Making
    predictions on validation data.\")\n        try:\n            y_pred_proba = model.predict_proba(X_valid)[:,
    1]\n            y_pred = model.predict(X_valid)\n        except Exception as e:\n
    \           logger.error(f\"Error during prediction: {e}\")\n            mlflow.set_tag(\"evaluation_status\",
    \"failed_prediction\")\n            raise\n\n        # Calculate metrics\n        logger.info(\"Calculating
    evaluation metrics.\")\n        try:\n            accuracy = accuracy_score(y_valid,
    y_pred)\n            auc = roc_auc_score(y_valid, y_pred_proba)\n            precision
    = precision_score(y_valid, y_pred, zero_division=0)\n            recall = recall_score(y_valid,
    y_pred, zero_division=0)\n            f1 = f1_score(y_valid, y_pred, zero_division=0)\n\n
    \           metrics = {\n                \"accuracy\": accuracy,\n                \"auc\":
    auc,\n                \"precision\": precision,\n                \"recall\": recall,\n
    \               \"f1_score\": f1\n            }\n            logger.info(f\"Evaluation
    Metrics: {metrics}\")\n\n            # Log metrics to MLflow\n            logger.info(\"Logging
    metrics to MLflow.\")\n            mlflow.log_metrics(metrics)\n\n            #
    Print metrics for Argo logs / SageMaker HPO compatibility\n            print(f\"validation:accuracy:
    {accuracy}\")\n            print(f\"validation:auc: {auc}\")\n            print(f\"validation:precision:
    {precision}\")\n            print(f\"validation:recall: {recall}\")\n            print(f\"validation:f1_score:
    {f1}\")\n\n            # Save metrics to JSON file\n            logger.info(f\"Saving
    evaluation metrics to {metrics_output_path}\")\n            os.makedirs(os.path.dirname(metrics_output_path),
    exist_ok=True)\n            with open(metrics_output_path, 'w') as f:\n                json.dump(metrics,
    f, indent=4)\n            logger.info(\"Metrics saved successfully.\")\n            mlflow.log_artifact(metrics_output_path,
    artifact_path=\"evaluation_results\")\n            mlflow.set_tag(\"evaluation_status\",
    \"completed\")\n\n        except Exception as e:\n            logger.error(f\"Error
    during metrics calculation or logging: {e}\")\n            mlflow.set_tag(\"evaluation_status\",
    \"failed_metrics_calculation\")\n            raise\n\n    logger.info(\"Model
    evaluation script finished.\")\n    return metrics\n\n\nif __name__ == '__main__':\n
    \   parser = argparse.ArgumentParser(description=\"Evaluate a trained XGBoost
    model.\")\n    parser.add_argument('--model-path', type=str, required=True,\n
    \                       help='Path to the trained XGBoost model file (e.g., /opt/ml/processing/model/xgboost-model).')\n
    \   parser.add_argument('--valid-data-path', type=str, required=True,\n                        help='Path
    to the validation data CSV file (e.g., /opt/ml/processing/output/test/test.csv).')\n
    \   parser.add_argument('--metrics-output-path', type=str, required=True,\n                        help='Path
    to save the evaluation metrics JSON file (e.g., /opt/ml/processing/output/evaluation_metrics.json).')\n
    \   parser.add_argument('--mlflow-experiment-name', type=str, default=\"Churn_Prediction_XGBoost\",
    # Or a new one like \"Churn_Model_Evaluation\"\n                        help='Name
    of the MLflow experiment to log metrics to.')\n    parser.add_argument('--training-run-id',
    type=str, default=None, required=False,\n                        help='(Optional)
    MLflow Run ID of the training job to associate this evaluation with.')\n    \n
    \   args = parser.parse_args()\n\n    # Ensure MLFLOW_TRACKING_URI is set in the
    environment by Argo workflow\n    if not os.environ.get(\"MLFLOW_TRACKING_URI\"):\n
    \       logger.warning(\"MLFLOW_TRACKING_URI environment variable not set. MLflow
    logging might fail or use local tracking.\")\n\n    evaluate_model(\n        model_path=args.model_path,\n
    \       valid_data_path=args.valid_data_path,\n        metrics_output_path=args.metrics_output_path,\n
    \       mlflow_experiment_name=args.mlflow_experiment_name,\n        training_run_id=args.training_run_id\n
    \   )"
  preprocessing.py: "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder
    # Keep if used, though current script doesn't use it for final encoding\nfrom
    sklearn.model_selection import train_test_split\nimport argparse\nimport os\nimport
    mlflow\nimport logging\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef
    main(args):\n    # --- MLflow Setup ---\n    # It's good practice to set the tracking
    URI via an environment variable\n    # or have it configured globally for your
    MLflow server.\n    # Defaulting here for standalone script execution, but override
    in your Argo workflow.\n    mlflow_tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\",
    \"http://localhost:5000\") # Replace with your MLflow server URI\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n
    \   \n    try:\n        mlflow.set_experiment(args.mlflow_experiment_name)\n        logger.info(f\"Using
    MLflow experiment: {args.mlflow_experiment_name}\")\n    except Exception as e:\n
    \       logger.error(f\"Could not set MLflow experiment '{args.mlflow_experiment_name}':
    {e}\")\n        # Fallback or create if not exists (more robust)\n        try:\n
    \           logger.info(f\"Attempting to create MLflow experiment: {args.mlflow_experiment_name}\")\n
    \           mlflow.create_experiment(args.mlflow_experiment_name)\n            mlflow.set_experiment(args.mlflow_experiment_name)\n
    \       except Exception as e_create:\n            logger.error(f\"Failed to create
    or set MLflow experiment: {e_create}\")\n            # Decide if you want to proceed
    without MLflow or raise error\n            # For now, we'll try to proceed, but
    logging might fail.\n\n    with mlflow.start_run(run_name=\"preprocessing_run\")
    as run:\n        run_id = run.info.run_id\n        logger.info(f\"MLflow Run ID:
    {run_id}\")\n        logger.info(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n
    \       logger.info(f\"MLflow Artifact URI: {mlflow.get_artifact_uri()}\")\n\n
    \       # Log parameters\n        mlflow.log_param(\"input_data_path\", args.input_data_path)\n
    \       mlflow.log_param(\"test_split_ratio\", args.test_split_ratio)\n        mlflow.log_param(\"random_state\",
    args.random_state)\n        mlflow.log_param(\"output_train_path\", args.output_train_path)\n
    \       mlflow.log_param(\"output_test_path\", args.output_test_path)\n\n        #
    --- Original Data Processing Logic (with minor adjustments for paths) ---\n        logger.info(f\"Reading
    data from: {args.input_data_path}\")\n        df = pd.read_csv(args.input_data_path)\n
    \       \n        logger.info(\"Dropping 'customerID' column.\")\n        df =
    df.drop(['customerID'], axis=1)\n        \n        logger.info(\"Converting 'TotalCharges'
    to numeric and filling NaNs with 0.\")\n        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],
    errors='coerce').fillna(0)\n\n        # Original script's LabelEncoder loop -
    this encodes all object columns.\n        # This is different from the one-hot
    encoding in the more detailed script I provided earlier.\n        # Sticking to
    the user's original logic here.\n        le = LabelEncoder()\n        logger.info(\"Label
    encoding object type columns.\")\n        for col in df.select_dtypes(include=['object']).columns:\n
    \           df[col] = le.fit_transform(df[col])\n\n        X = df.drop('Churn',
    axis=1)\n        y = df['Churn']\n        \n        logger.info(f\"Splitting data
    with test_size={args.test_split_ratio} and random_state={args.random_state}\")\n
    \       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_split_ratio,
    random_state=args.random_state)\n\n        train_df = pd.concat([y_train.reset_index(drop=True),
    X_train.reset_index(drop=True)], axis=1)\n        test_df = pd.concat([y_test.reset_index(drop=True),
    X_test.reset_index(drop=True)], axis=1)\n        \n        # Ensure output directories
    exist\n        os.makedirs(os.path.dirname(args.output_train_path), exist_ok=True)\n
    \       os.makedirs(os.path.dirname(args.output_test_path), exist_ok=True)\n\n
    \       logger.info(f\"Saving training data to: {args.output_train_path}\")\n
    \       train_df.to_csv(args.output_train_path, index=False)\n        \n        logger.info(f\"Saving
    test data to: {args.output_test_path}\")\n        test_df.to_csv(args.output_test_path,
    index=False)\n\n        # --- Log processed data as MLflow artifacts ---\n        logger.info(\"Logging
    processed train.csv and test.csv as MLflow artifacts.\")\n        mlflow.log_artifact(args.output_train_path,
    artifact_path=\"processed_data\")\n        mlflow.log_artifact(args.output_test_path,
    artifact_path=\"processed_data\")\n        \n        mlflow.set_tag(\"preprocessing_status\",
    \"completed\")\n        logger.info(\"Preprocessing script finished successfully.\")\n\nif
    __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Preprocess
    churn data and log to MLflow.\")\n    parser.add_argument('--input-data-path',
    type=str, default='/opt/ml/processing/input/WA_Fn-UseC_-Telco-Customer-Churn.csv',
    help='Path to the input CSV file.')\n    parser.add_argument('--output-train-path',
    type=str, default='/opt/ml/processing/output/train/train.csv', help='Path to save
    the processed training data.')\n    parser.add_argument('--output-test-path',
    type=str, default='/opt/ml/processing/output/test/test.csv', help='Path to save
    the processed test data.')\n    parser.add_argument('--test-split-ratio', type=float,
    default=0.2, help='Ratio for train-test split.')\n    parser.add_argument('--random-state',
    type=int, default=42, help='Random state for train-test split.')\n    parser.add_argument('--mlflow-experiment-name',
    type=str, default=\"Churn_Prediction_Experiment\", help=\"Name of the MLflow experiment.\")\n
    \   \n    args = parser.parse_args()\n    main(args)\n"
  xgboost_script.py: "import xgboost as xgb\nimport pandas as pd\nimport os\nimport
    logging\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport
    argparse # Added for MLflow experiment name\nimport mlflow # Added for MLflow\nimport
    mlflow.xgboost # Added for MLflow XGBoost integration\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nif
    __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    # Argument
    for MLflow experiment name\n    parser.add_argument('--mlflow-experiment-name',
    type=str, default=\"Churn_Prediction_XGBoost\", help=\"Name of the MLflow experiment.\")\n
    \   # Optional: Add arguments for data paths if you want to override SageMaker
    defaults when not in SageMaker\n    parser.add_argument('--train-data-path', type=str,
    default='/opt/ml/processing/input/data/train/train.csv')\n    parser.add_argument('--valid-data-path',
    type=str, default='/opt/ml/processing/input/data/validation/test.csv')\n    parser.add_argument('--model-output-path',
    type=str, default='/opt/ml/processing/model/xgboost-model') # SageMaker model
    path\n    parser.add_argument('--metrics-output-path', type=str, default='/opt/ml/processing/output/metrics.json')
    # SageMaker metrics path\n\n    args = parser.parse_args()\n\n    logger.info(\"Starting
    XGBoost training script.\")\n\n    # --- MLflow Setup ---\n    mlflow_tracking_uri
    = os.environ.get(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\") # Replace
    with your MLflow server URI\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n
    \   \n    try:\n        mlflow.set_experiment(args.mlflow_experiment_name)\n        logger.info(f\"Using
    MLflow experiment: {args.mlflow_experiment_name}\")\n    except Exception as e:\n
    \       logger.error(f\"Could not set MLflow experiment '{args.mlflow_experiment_name}':
    {e}\")\n        try:\n            logger.info(f\"Attempting to create MLflow experiment:
    {args.mlflow_experiment_name}\")\n            mlflow.create_experiment(args.mlflow_experiment_name)\n
    \           mlflow.set_experiment(args.mlflow_experiment_name)\n        except
    Exception as e_create:\n            logger.error(f\"Failed to create or set MLflow
    experiment: {e_create}\")\n\n\n    with mlflow.start_run(run_name=\"xgboost_training_run\")
    as run:\n        run_id = run.info.run_id\n        logger.info(f\"MLflow Run ID:
    {run_id}\")\n        logger.info(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n
    \       logger.info(f\"MLflow Artifact URI: {mlflow.get_artifact_uri()}\")\n\n
    \       # Load training data\n        logger.info(f\"Loading training data from
    {args.train_data_path}\")\n        train_data = pd.read_csv(args.train_data_path)\n
    \       X_train = train_data.iloc[:, 1:]\n        y_train = train_data.iloc[:,
    0]\n        logger.info(f\"Training data loaded. Shape: {train_data.shape}\")\n\n
    \       # Load validation data\n        logger.info(f\"Loading validation data
    from {args.valid_data_path}\")\n        valid_data = pd.read_csv(args.valid_data_path)\n
    \       X_valid = valid_data.iloc[:, 1:]\n        y_valid = valid_data.iloc[:,
    0]\n        logger.info(f\"Validation data loaded. Shape: {valid_data.shape}\")\n\n
    \       # Define hyperparameters, reading from SageMaker env vars or defaults\n
    \       hp_max_depth = int(float(os.environ.get('SM_HP_MAX_DEPTH', 5)))\n        hp_eta
    = float(os.environ.get('SM_HP_ETA', 0.2))\n        hp_min_child_weight = float(os.environ.get('SM_HP_MIN_CHILD_WEIGHT',
    1))\n        hp_subsample = float(os.environ.get('SM_HP_SUBSAMPLE', 0.8))\n        hp_num_round
    = int(os.environ.get('SM_HP_NUM_ROUND', 100)) # SageMaker often passes num_round\n\n
    \       model_params = {\n            'max_depth': hp_max_depth,\n            'eta':
    hp_eta,\n            'min_child_weight': hp_min_child_weight,\n            'subsample':
    hp_subsample,\n            'objective': 'binary:logistic',\n            'n_estimators':
    hp_num_round, # XGBClassifier uses n_estimators\n            'use_label_encoder':
    False # Suppress warning for newer XGBoost\n        }\n        \n        # Log
    parameters to MLflow\n        logger.info(f\"Logging parameters to MLflow: {model_params}\")\n
    \       mlflow.log_params({\n            'max_depth': hp_max_depth,\n            'eta':
    hp_eta,\n            'min_child_weight': hp_min_child_weight,\n            'subsample':
    hp_subsample,\n            'num_round': hp_num_round, # Logged as num_round for
    consistency with SM\n            'objective': 'binary:logistic'\n        })\n
    \       mlflow.log_param(\"train_data_path\", args.train_data_path)\n        mlflow.log_param(\"valid_data_path\",
    args.valid_data_path)\n\n\n        # Initialize XGBoost model\n        logger.info(\"Initializing
    XGBoost model with hyperparameters.\")\n        # XGBClassifier uses n_estimators
    instead of num_round directly in constructor\n        model = xgb.XGBClassifier(\n
    \           max_depth=hp_max_depth,\n            eta=hp_eta,\n            min_child_weight=hp_min_child_weight,\n
    \           subsample=hp_subsample,\n            objective='binary:logistic',\n
    \           n_estimators=hp_num_round, # Use n_estimators here\n            use_label_encoder=False
    # Suppress warning\n        )\n        logger.info(\"XGBoost model initialized.\")\n\n
    \       # Train the model\n        logger.info(\"Starting model training.\")\n
    \       model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n
    \       logger.info(\"Model training completed.\")\n\n        # Save the model
    (SageMaker conventional path) - kept for compatibility\n        logger.info(f\"Saving
    the trained model to SageMaker path: {args.model_output_path}\")\n        os.makedirs(os.path.dirname(args.model_output_path),
    exist_ok=True)\n        model.save_model(args.model_output_path)\n        logger.info(\"Model
    saved to SageMaker path successfully.\")\n\n        # Log model with MLflow\n
    \       logger.info(\"Logging model with MLflow.\")\n        mlflow.xgboost.log_model(\n
    \           xgb_model=model,\n            artifact_path=\"xgboost-churn-model\",
    # This will be a directory in MLflow artifacts\n            input_example=X_train.head(),
    # Optional: for signature inference\n            signature=mlflow.models.infer_signature(X_train,
    model.predict(X_train)) # Optional\n        )\n        logger.info(\"Model logged
    to MLflow successfully.\")\n\n        # Evaluate the model\n        logger.info(\"Evaluating
    the model on validation data.\")\n        y_pred = model.predict(X_valid)\n        y_pred_proba
    = model.predict_proba(X_valid)[:, 1]\n        accuracy = accuracy_score(y_valid,
    y_pred)\n        auc = roc_auc_score(y_valid, y_pred_proba)\n        logger.info(f\"Model
    evaluation completed. Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n\n        #
    Log metrics to MLflow\n        logger.info(\"Logging metrics to MLflow.\")\n        mlflow.log_metric(\"validation_accuracy\",
    accuracy)\n        mlflow.log_metric(\"validation_auc\", auc)\n\n        # Print
    metrics in the format expected by SageMaker HPO - kept for compatibility\n        print(f\"validation:accuracy:
    {accuracy}\")\n        print(f\"validation:auc: {auc}\")\n\n        # Save metrics
    to file (SageMaker conventional path) - kept for compatibility\n        logger.info(f\"Saving
    evaluation metrics to SageMaker path: {args.metrics_output_path}\")\n        os.makedirs(os.path.dirname(args.metrics_output_path),
    exist_ok=True)\n        metrics_content = {'accuracy': accuracy, 'auc': auc}\n
    \       with open(args.metrics_output_path, 'w') as f:\n            json.dump(metrics_content,
    f)\n        logger.info(\"Metrics saved to SageMaker path successfully.\")\n        \n
    \       mlflow.set_tag(\"training_status\", \"completed\")\n        logger.info(\"XGBoost
    training script finished.\")\n"
kind: ConfigMap
metadata:
  name: churn-pipeline-scripts
  namespace: argowf
