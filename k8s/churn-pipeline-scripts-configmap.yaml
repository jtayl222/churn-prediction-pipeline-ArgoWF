apiVersion: v1
data:
  preprocessing.py: "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder
    # Keep if used, though current script doesn't use it for final encoding\nfrom
    sklearn.model_selection import train_test_split\nimport argparse\nimport os\nimport
    mlflow\nimport logging\n\n# Configure basic logging\nlogging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef
    main(args):\n    # --- MLflow Setup ---\n    # It's good practice to set the tracking
    URI via an environment variable\n    # or have it configured globally for your
    MLflow server.\n    # Defaulting here for standalone script execution, but override
    in your Argo workflow.\n    mlflow_tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\",
    \"http://localhost:5000\") # Replace with your MLflow server URI\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n
    \   \n    try:\n        mlflow.set_experiment(args.mlflow_experiment_name)\n        logger.info(f\"Using
    MLflow experiment: {args.mlflow_experiment_name}\")\n    except Exception as e:\n
    \       logger.error(f\"Could not set MLflow experiment '{args.mlflow_experiment_name}':
    {e}\")\n        # Fallback or create if not exists (more robust)\n        try:\n
    \           logger.info(f\"Attempting to create MLflow experiment: {args.mlflow_experiment_name}\")\n
    \           mlflow.create_experiment(args.mlflow_experiment_name)\n            mlflow.set_experiment(args.mlflow_experiment_name)\n
    \       except Exception as e_create:\n            logger.error(f\"Failed to create
    or set MLflow experiment: {e_create}\")\n            # Decide if you want to proceed
    without MLflow or raise error\n            # For now, we'll try to proceed, but
    logging might fail.\n\n    with mlflow.start_run(run_name=\"preprocessing_run\")
    as run:\n        run_id = run.info.run_id\n        logger.info(f\"MLflow Run ID:
    {run_id}\")\n        logger.info(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n
    \       logger.info(f\"MLflow Artifact URI: {mlflow.get_artifact_uri()}\")\n\n
    \       # Log parameters\n        mlflow.log_param(\"input_data_path\", args.input_data_path)\n
    \       mlflow.log_param(\"test_split_ratio\", args.test_split_ratio)\n        mlflow.log_param(\"random_state\",
    args.random_state)\n        mlflow.log_param(\"output_train_path\", args.output_train_path)\n
    \       mlflow.log_param(\"output_test_path\", args.output_test_path)\n\n        #
    --- Original Data Processing Logic (with minor adjustments for paths) ---\n        logger.info(f\"Reading
    data from: {args.input_data_path}\")\n        df = pd.read_csv(args.input_data_path)\n
    \       \n        logger.info(\"Dropping 'customerID' column.\")\n        df =
    df.drop(['customerID'], axis=1)\n        \n        logger.info(\"Converting 'TotalCharges'
    to numeric and filling NaNs with 0.\")\n        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],
    errors='coerce').fillna(0)\n\n        # Original script's LabelEncoder loop -
    this encodes all object columns.\n        # This is different from the one-hot
    encoding in the more detailed script I provided earlier.\n        # Sticking to
    the user's original logic here.\n        le = LabelEncoder()\n        logger.info(\"Label
    encoding object type columns.\")\n        for col in df.select_dtypes(include=['object']).columns:\n
    \           df[col] = le.fit_transform(df[col])\n\n        X = df.drop('Churn',
    axis=1)\n        y = df['Churn']\n        \n        logger.info(f\"Splitting data
    with test_size={args.test_split_ratio} and random_state={args.random_state}\")\n
    \       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_split_ratio,
    random_state=args.random_state)\n\n        train_df = pd.concat([y_train.reset_index(drop=True),
    X_train.reset_index(drop=True)], axis=1)\n        test_df = pd.concat([y_test.reset_index(drop=True),
    X_test.reset_index(drop=True)], axis=1)\n        \n        # Ensure output directories
    exist\n        os.makedirs(os.path.dirname(args.output_train_path), exist_ok=True)\n
    \       os.makedirs(os.path.dirname(args.output_test_path), exist_ok=True)\n\n
    \       logger.info(f\"Saving training data to: {args.output_train_path}\")\n
    \       train_df.to_csv(args.output_train_path, index=False)\n        \n        logger.info(f\"Saving
    test data to: {args.output_test_path}\")\n        test_df.to_csv(args.output_test_path,
    index=False)\n\n        # --- Log processed data as MLflow artifacts ---\n        logger.info(\"Logging
    processed train.csv and test.csv as MLflow artifacts.\")\n        mlflow.log_artifact(args.output_train_path,
    artifact_path=\"processed_data\")\n        mlflow.log_artifact(args.output_test_path,
    artifact_path=\"processed_data\")\n        \n        mlflow.set_tag(\"preprocessing_status\",
    \"completed\")\n        logger.info(\"Preprocessing script finished successfully.\")\n\nif
    __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Preprocess
    churn data and log to MLflow.\")\n    parser.add_argument('--input-data-path',
    type=str, default='/opt/ml/processing/input/WA_Fn-UseC_-Telco-Customer-Churn.csv',
    help='Path to the input CSV file.')\n    parser.add_argument('--output-train-path',
    type=str, default='/opt/ml/processing/output/train/train.csv', help='Path to save
    the processed training data.')\n    parser.add_argument('--output-test-path',
    type=str, default='/opt/ml/processing/output/test/test.csv', help='Path to save
    the processed test data.')\n    parser.add_argument('--test-split-ratio', type=float,
    default=0.2, help='Ratio for train-test split.')\n    parser.add_argument('--random-state',
    type=int, default=42, help='Random state for train-test split.')\n    parser.add_argument('--mlflow-experiment-name',
    type=str, default=\"Churn_Prediction_Experiment\", help=\"Name of the MLflow experiment.\")\n
    \   \n    args = parser.parse_args()\n    main(args)\n"
  xgboost_script.py: "import xgboost as xgb\nimport pandas as pd\nimport os\nimport
    logging\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport
    argparse # Added for MLflow experiment name\nimport mlflow # Added for MLflow\nimport
    mlflow.xgboost # Added for MLflow XGBoost integration\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nif
    __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    # Argument
    for MLflow experiment name\n    parser.add_argument('--mlflow-experiment-name',
    type=str, default=\"Churn_Prediction_XGBoost\", help=\"Name of the MLflow experiment.\")\n
    \   # Optional: Add arguments for data paths if you want to override SageMaker
    defaults when not in SageMaker\n    parser.add_argument('--train-data-path', type=str,
    default='/opt/ml/processing/input/data/train/train.csv')\n    parser.add_argument('--valid-data-path',
    type=str, default='/opt/ml/processing/input/data/validation/test.csv')\n    parser.add_argument('--model-output-path',
    type=str, default='/opt/ml/processing/model/xgboost-model') # SageMaker model
    path\n    parser.add_argument('--metrics-output-path', type=str, default='/opt/ml/processing/output/metrics.json')
    # SageMaker metrics path\n\n    args = parser.parse_args()\n\n    logger.info(\"Starting
    XGBoost training script.\")\n\n    # --- MLflow Setup ---\n    mlflow_tracking_uri
    = os.environ.get(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\") # Replace
    with your MLflow server URI\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n
    \   \n    try:\n        mlflow.set_experiment(args.mlflow_experiment_name)\n        logger.info(f\"Using
    MLflow experiment: {args.mlflow_experiment_name}\")\n    except Exception as e:\n
    \       logger.error(f\"Could not set MLflow experiment '{args.mlflow_experiment_name}':
    {e}\")\n        try:\n            logger.info(f\"Attempting to create MLflow experiment:
    {args.mlflow_experiment_name}\")\n            mlflow.create_experiment(args.mlflow_experiment_name)\n
    \           mlflow.set_experiment(args.mlflow_experiment_name)\n        except
    Exception as e_create:\n            logger.error(f\"Failed to create or set MLflow
    experiment: {e_create}\")\n\n\n    with mlflow.start_run(run_name=\"xgboost_training_run\")
    as run:\n        run_id = run.info.run_id\n        logger.info(f\"MLflow Run ID:
    {run_id}\")\n        logger.info(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n
    \       logger.info(f\"MLflow Artifact URI: {mlflow.get_artifact_uri()}\")\n\n
    \       # Load training data\n        logger.info(f\"Loading training data from
    {args.train_data_path}\")\n        train_data = pd.read_csv(args.train_data_path)\n
    \       X_train = train_data.iloc[:, 1:]\n        y_train = train_data.iloc[:,
    0]\n        logger.info(f\"Training data loaded. Shape: {train_data.shape}\")\n\n
    \       # Load validation data\n        logger.info(f\"Loading validation data
    from {args.valid_data_path}\")\n        valid_data = pd.read_csv(args.valid_data_path)\n
    \       X_valid = valid_data.iloc[:, 1:]\n        y_valid = valid_data.iloc[:,
    0]\n        logger.info(f\"Validation data loaded. Shape: {valid_data.shape}\")\n\n
    \       # Define hyperparameters, reading from SageMaker env vars or defaults\n
    \       hp_max_depth = int(float(os.environ.get('SM_HP_MAX_DEPTH', 5)))\n        hp_eta
    = float(os.environ.get('SM_HP_ETA', 0.2))\n        hp_min_child_weight = float(os.environ.get('SM_HP_MIN_CHILD_WEIGHT',
    1))\n        hp_subsample = float(os.environ.get('SM_HP_SUBSAMPLE', 0.8))\n        hp_num_round
    = int(os.environ.get('SM_HP_NUM_ROUND', 100)) # SageMaker often passes num_round\n\n
    \       model_params = {\n            'max_depth': hp_max_depth,\n            'eta':
    hp_eta,\n            'min_child_weight': hp_min_child_weight,\n            'subsample':
    hp_subsample,\n            'objective': 'binary:logistic',\n            'n_estimators':
    hp_num_round, # XGBClassifier uses n_estimators\n            'use_label_encoder':
    False # Suppress warning for newer XGBoost\n        }\n        \n        # Log
    parameters to MLflow\n        logger.info(f\"Logging parameters to MLflow: {model_params}\")\n
    \       mlflow.log_params({\n            'max_depth': hp_max_depth,\n            'eta':
    hp_eta,\n            'min_child_weight': hp_min_child_weight,\n            'subsample':
    hp_subsample,\n            'num_round': hp_num_round, # Logged as num_round for
    consistency with SM\n            'objective': 'binary:logistic'\n        })\n
    \       mlflow.log_param(\"train_data_path\", args.train_data_path)\n        mlflow.log_param(\"valid_data_path\",
    args.valid_data_path)\n\n\n        # Initialize XGBoost model\n        logger.info(\"Initializing
    XGBoost model with hyperparameters.\")\n        # XGBClassifier uses n_estimators
    instead of num_round directly in constructor\n        model = xgb.XGBClassifier(\n
    \           max_depth=hp_max_depth,\n            eta=hp_eta,\n            min_child_weight=hp_min_child_weight,\n
    \           subsample=hp_subsample,\n            objective='binary:logistic',\n
    \           n_estimators=hp_num_round, # Use n_estimators here\n            use_label_encoder=False
    # Suppress warning\n        )\n        logger.info(\"XGBoost model initialized.\")\n\n
    \       # Train the model\n        logger.info(\"Starting model training.\")\n
    \       model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n
    \       logger.info(\"Model training completed.\")\n\n        # Save the model
    (SageMaker conventional path) - kept for compatibility\n        logger.info(f\"Saving
    the trained model to SageMaker path: {args.model_output_path}\")\n        os.makedirs(os.path.dirname(args.model_output_path),
    exist_ok=True)\n        model.save_model(args.model_output_path)\n        logger.info(\"Model
    saved to SageMaker path successfully.\")\n\n        # Log model with MLflow\n
    \       logger.info(\"Logging model with MLflow.\")\n        mlflow.xgboost.log_model(\n
    \           xgb_model=model,\n            artifact_path=\"xgboost-churn-model\",
    # This will be a directory in MLflow artifacts\n            input_example=X_train.head(),
    # Optional: for signature inference\n            signature=mlflow.models.infer_signature(X_train,
    model.predict(X_train)) # Optional\n        )\n        logger.info(\"Model logged
    to MLflow successfully.\")\n\n        # Evaluate the model\n        logger.info(\"Evaluating
    the model on validation data.\")\n        y_pred = model.predict(X_valid)\n        y_pred_proba
    = model.predict_proba(X_valid)[:, 1]\n        accuracy = accuracy_score(y_valid,
    y_pred)\n        auc = roc_auc_score(y_valid, y_pred_proba)\n        logger.info(f\"Model
    evaluation completed. Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n\n        #
    Log metrics to MLflow\n        logger.info(\"Logging metrics to MLflow.\")\n        mlflow.log_metric(\"validation_accuracy\",
    accuracy)\n        mlflow.log_metric(\"validation_auc\", auc)\n\n        # Print
    metrics in the format expected by SageMaker HPO - kept for compatibility\n        print(f\"validation:accuracy:
    {accuracy}\")\n        print(f\"validation:auc: {auc}\")\n\n        # Save metrics
    to file (SageMaker conventional path) - kept for compatibility\n        logger.info(f\"Saving
    evaluation metrics to SageMaker path: {args.metrics_output_path}\")\n        os.makedirs(os.path.dirname(args.metrics_output_path),
    exist_ok=True)\n        metrics_content = {'accuracy': accuracy, 'auc': auc}\n
    \       with open(args.metrics_output_path, 'w') as f:\n            json.dump(metrics_content,
    f)\n        logger.info(\"Metrics saved to SageMaker path successfully.\")\n        \n
    \       mlflow.set_tag(\"training_status\", \"completed\")\n        logger.info(\"XGBoost
    training script finished.\")\n"
kind: ConfigMap
metadata:
  name: churn-pipeline-scripts
  namespace: argowf
